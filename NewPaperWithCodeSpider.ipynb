{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN4T8YoSfqNP6+iYU4Fvp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/awesome-colab-project/blob/main/NewPaperWithCodeSpider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "新版的paper with code爬虫，增加GPT自动总结的功能\n",
        "\n",
        "implemented by 李鲁鲁"
      ],
      "metadata": {
        "id": "3yCj2XI0GyOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q paperswithcode-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftqKbG45Hwx1",
        "outputId": "1ab5228a-e7aa-4db7-86fc-46687d7cef57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.6/195.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.9/470.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m674.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "sqlalchemy 2.0.24 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "confection 0.1.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "flax 0.7.5 requires rich>=11.1, but you have rich 9.11.1 which is incompatible.\n",
            "flax 0.7.5 requires typing-extensions>=4.2, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "ibis-framework 7.1.0 requires pytz>=2022.7, but you have pytz 2021.3 which is incompatible.\n",
            "ibis-framework 7.1.0 requires rich<14,>=12.4.4, but you have rich 9.11.1 which is incompatible.\n",
            "ibis-framework 7.1.0 requires typing-extensions<5,>=4.3.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.6.2 which is incompatible.\n",
            "librosa 0.10.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\n",
            "polars 0.17.3 requires typing_extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "python-utils 3.8.1 requires typing-extensions>3.10.0.2, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "spacy 3.6.1 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
            "thinc 8.1.12 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
            "yfinance 0.2.33 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_name = '/content/drive/MyDrive/paper_with_code.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTmHAYdmKpuv",
        "outputId": "3e935e35-566a-4bf3-9664-983ef5f32833"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "previous_datas = []\n",
        "\n",
        "with open(save_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        previous_datas.append(json.loads(line))\n",
        "        tmp = json.loads(line)\n",
        "        visited_urls.add(tmp['url'])\n",
        "\n",
        "minimal_stars = previous_datas[-1]['stars']\n"
      ],
      "metadata": {
        "id": "M1nJ9FYXSe5p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(previous_datas[-1].keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L5lIGzPSumy",
        "outputId": "2a26160c-31be-472b-a5da-e7622349c858"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['stars', 'url', 'description'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/paper_with_code"
      ],
      "metadata": {
        "id": "AKt15WpZUbJp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url2id = {}\n",
        "\n",
        "for i, data in enumerate(previous_datas):\n",
        "    url2id[data['url']] = i"
      ],
      "metadata": {
        "id": "_0oTS1WuUqJ8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from paperswithcode import PapersWithCodeClient\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "client = PapersWithCodeClient()\n",
        "\n",
        "save_name = '/content/drive/MyDrive/paper_with_code/0106.txt'\n",
        "# save_name = '/content/output.txt'\n",
        "\n",
        "\n",
        "written_in_this_time_scan = set()\n",
        "for page_id in range(1,300):\n",
        "    if page_id == 1:\n",
        "        open_format = 'w'\n",
        "    else:\n",
        "        open_format = 'a'\n",
        "    with open(save_name, open_format, encoding='utf-8') as f:\n",
        "\n",
        "        papers = client.repository_list(stars=50, items_per_page=500, page=page_id)\n",
        "\n",
        "        print('dealing with page ', page_id)\n",
        "\n",
        "\n",
        "        for data in tqdm(papers.results):\n",
        "            my_dict = {}\n",
        "            my_dict['stars'] = data.stars\n",
        "\n",
        "            my_dict['url'] = data.url\n",
        "\n",
        "            if data.url in written_in_this_time_scan:\n",
        "                continue\n",
        "\n",
        "            if data.url in url2id:\n",
        "                new_stars = data.stars\n",
        "                index = url2id[data.url]\n",
        "                old_stars = previous_datas[index]['stars']\n",
        "                if new_stars > old_stars:\n",
        "                    previous_datas[index]['stars'] = new_stars\n",
        "                    previous_datas[index]['increased_stars'] = new_stars - old_stars\n",
        "                json_str = json.dumps(previous_datas[index], ensure_ascii=False)\n",
        "                f.write(json_str+'\\n')\n",
        "                written_in_this_time_scan.add(data.url)\n",
        "                continue\n",
        "\n",
        "\n",
        "            written_in_this_time_scan.add(data.url)\n",
        "\n",
        "            page = requests.get(data.url)\n",
        "\n",
        "            if 'colab' not in page.text and 'Colab' not in page.text and 'HuggingFace' not in page.text and 'huggingface' not in page.text:\n",
        "                continue\n",
        "\n",
        "            my_dict['description'] = data.description\n",
        "            json_str = json.dumps(my_dict, ensure_ascii=False)\n",
        "            f.write(json_str+'\\n')"
      ],
      "metadata": {
        "id": "Ri6PQrMFNTva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9V0hp6TTVoj",
        "outputId": "ea3c4a65-73af-4b68-fc12-ffde39b67004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/kornia/kornia/blob/master/kornia/feature/siftdesc.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(papers.results[1])\n",
        "print(papers.results[1].url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPz8Mz35Ncch",
        "outputId": "7e654566-1d2e-40cb-b01c-57603674445f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url='https://github.com/sindresorhus/awesome' owner='sindresorhus' name='awesome' description='😎 Awesome lists about all kinds of interesting topics' stars=268030 framework='tf' is_official=None\n",
            "https://github.com/sindresorhus/awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from paperswithcode import PapersWithCodeClient\n",
        "import json\n",
        "\n",
        "client = PapersWithCodeClient()\n",
        "\n",
        "# save_name = '/content/output.txt'\n",
        "\n",
        "with open(save_name, 'w', encoding='utf-8') as f:\n",
        "\n",
        "  for page_id in range(1,200):\n",
        "\n",
        "    papers = client.repository_list(stars=100, items_per_page=500, page=page_id)\n",
        "\n",
        "    print('dealing page ', page_id)\n",
        "\n",
        "    for i in range(len(papers.results)):\n",
        "\n",
        "      res = papers.results[i]\n",
        "\n",
        "      try_count = 0\n",
        "      paper_list = []\n",
        "\n",
        "      while try_count < 3:\n",
        "\n",
        "        try:\n",
        "          paper_list = client.repository_paper_list(owner=res.owner, name=res.name)\n",
        "          time.sleep(0.1)\n",
        "          try_count += 10\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        if paper_list != []:\n",
        "          break\n",
        "\n",
        "      if not paper_list:\n",
        "        continue\n",
        "\n",
        "      my_url = getattr(papers.results[i],'url','')\n",
        "      page = requests.get(my_url)\n",
        "\n",
        "      if 'colab' not in page.text and 'Colab' not in page.text:\n",
        "        continue\n",
        "\n",
        "      my_dict = {}\n",
        "      my_dict['stars'] = getattr(papers.results[i],'stars','')\n",
        "      my_dict['name'] = getattr(papers.results[i],'name','')\n",
        "      my_dict['url'] = getattr(papers.results[i],'url','')\n",
        "      my_dict['description'] = getattr(papers.results[i],'description','')\n",
        "      json_str = json.dumps(my_dict)\n",
        "      f.write(json_str+'\\n')\n",
        "      print(json_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJqzUmX4I3Y6",
        "outputId": "a50daf02-e1b3-40b7-911f-0af0b877c67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dealing page  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G961dVE8KEns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "给出top15的分析"
      ],
      "metadata": {
        "id": "nzK9sjdptNjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "datas = []\n",
        "\n",
        "with open(save_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        datas.append(json.loads(line))\n"
      ],
      "metadata": {
        "id": "N5vM7p7etO22"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(datas))"
      ],
      "metadata": {
        "id": "GYbhZKKCtbp0",
        "outputId": "340b3dae-f4ed-4b6f-a29a-16420b6701d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9D2L5IWDuwpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_projs = []\n",
        "\n",
        "laplation_factor = 1000\n",
        "\n",
        "for i,data in enumerate(datas):\n",
        "    if \"increased_stars\" not in data:\n",
        "        # 计算ratio值\n",
        "        new_projs.append((data[\"stars\"], i))\n",
        "\n",
        "# 根据ratio排序\n",
        "new_projs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# 取前15个\n",
        "top15 = new_projs[:45]\n",
        "\n",
        "# 打印data\n",
        "for ratio, index in top15:\n",
        "    print(datas[index])\n",
        "\n"
      ],
      "metadata": {
        "id": "mb-UsKCHu87a",
        "outputId": "2da4519d-79b7-4397-a04b-494514b6890b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'stars': 114851, 'url': 'https://github.com/avelino/awesome-go', 'description': 'A curated list of awesome Go frameworks, libraries and software'}\n",
            "{'stars': 62273, 'url': 'https://github.com/josephmisiti/awesome-machine-learning', 'description': 'A curated list of awesome Machine Learning frameworks, libraries and software.'}\n",
            "{'stars': 60180, 'url': 'https://github.com/keras-team/keras', 'description': 'Deep Learning for humans'}\n",
            "{'stars': 60180, 'url': 'https://github.com/fchollet/keras', 'description': 'Deep Learning for humans'}\n",
            "{'stars': 44938, 'url': 'https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md', 'description': '🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 44936, 'url': 'https://github.com/huggingface/transformers/tree/master/examples/question-answering', 'description': '🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 37934, 'url': 'https://github.com/huggingface/transformers/tree/master/examples/movement-pruning', 'description': '🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 37859, 'url': 'https://github.com/killianlucas/open-interpreter', 'description': 'A natural language interface for computers'}\n",
            "{'stars': 36726, 'url': 'https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_squeezebert.py', 'description': '🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 33342, 'url': 'https://github.com/geekan/metagpt', 'description': '🌟 The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo'}\n",
            "{'stars': 31911, 'url': 'https://github.com/google-research/google-research/tree/master/blur', 'description': 'Google Research'}\n",
            "{'stars': 31906, 'url': 'https://github.com/google-research/google-research/tree/master/muller', 'description': 'Google Research'}\n",
            "{'stars': 31906, 'url': 'https://github.com/google-research/google-research/tree/master/memory_efficient_attention', 'description': 'Google Research'}\n",
            "{'stars': 28741, 'url': 'https://github.com/lllyasviel/fooocus', 'description': 'Focus on prompting and generating'}\n",
            "{'stars': 26751, 'url': 'https://github.com/jerryjliu/gpt_index', 'description': 'LlamaIndex (formerly GPT Index) is a data framework for your LLM applications'}\n",
            "{'stars': 26751, 'url': 'https://github.com/jerryjliu/llama_index', 'description': 'LlamaIndex (formerly GPT Index) is a data framework for your LLM applications'}\n",
            "{'stars': 26744, 'url': 'https://github.com/run-llama/llama_index', 'description': 'LlamaIndex (formerly GPT Index) is a data framework for your LLM applications'}\n",
            "{'stars': 25751, 'url': 'https://github.com/coqui-ai/TTS', 'description': '🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production'}\n",
            "{'stars': 25343, 'url': 'https://github.com/openbb-finance/openbbterminal', 'description': 'Investment Research for Everyone, Everywhere.'}\n",
            "{'stars': 21113, 'url': 'https://github.com/chatchat-space/langchain-chatchat', 'description': 'Langchain-Chatchat（原Langchain-ChatGLM）基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答 | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain '}\n",
            "{'stars': 19605, 'url': 'https://github.com/mindsdb/mindsdb', 'description': 'Build AI 🤖 using SQL'}\n",
            "{'stars': 15578, 'url': 'https://github.com/apple/ml-stable-diffusion', 'description': 'Stable Diffusion with Core ML on Apple Silicon'}\n",
            "{'stars': 14182, 'url': 'https://github.com/huggingface/datasets/tree/master/datasets/cppe-5', 'description': '🤗 The largest hub of ready-to-use datasets for ML models with fast, easy-to-use and efficient data manipulation tools'}\n",
            "{'stars': 14162, 'url': 'https://github.com/logspace-ai/langflow', 'description': '⛓️ Langflow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.'}\n",
            "{'stars': 13885, 'url': 'https://github.com/karpathy/llama2.c', 'description': 'Inference Llama 2 in one file of pure C'}\n",
            "{'stars': 13532, 'url': 'https://github.com/w-okada/voice-changer', 'description': 'リアルタイムボイスチェンジャー Realtime Voice Changer'}\n",
            "{'stars': 13001, 'url': 'https://github.com/openai/evals', 'description': 'Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.'}\n",
            "{'stars': 12595, 'url': 'https://github.com/sunner/chatall', 'description': ' Concurrently chat with ChatGPT, Bing Chat, Bard, Alpaca, Vicuna, Claude, ChatGLM, MOSS, 讯飞星火, 文心一言 and more, discover the best answers'}\n",
            "{'stars': 12390, 'url': 'https://github.com/myshell-ai/openvoice', 'description': 'Instant voice cloning by MyShell.'}\n",
            "{'stars': 11677, 'url': 'https://github.com/ydataai/ydata-profiling', 'description': '1 Line of code data quality profiling & exploratory data analysis for Pandas and Spark DataFrames. '}\n",
            "{'stars': 11293, 'url': 'https://github.com/hannibal046/awesome-llm', 'description': 'Awesome-LLM: a curated list of Large Language Model'}\n",
            "{'stars': 11236, 'url': 'https://github.com/google-deepmind/alphafold', 'description': 'Open source code for AlphaFold.'}\n",
            "{'stars': 9808, 'url': 'https://github.com/hiyouga/llama-factory', 'description': 'Easy-to-use LLM fine-tuning framework (LLaMA, BLOOM, Mistral, Baichuan, Qwen, ChatGLM)'}\n",
            "{'stars': 9469, 'url': 'https://github.com/gventuri/pandas-ai', 'description': 'Chat with your data (CSV, pandas, polars, etc). PandasAI makes data analysis conversational'}\n",
            "{'stars': 9371, 'url': 'https://github.com/eosphoros-ai/db-gpt', 'description': 'Revolutionizing Database Interactions with Private LLM Technology'}\n",
            "{'stars': 8961, 'url': 'https://github.com/redditsota/state-of-the-art-result-for-machine-learning-problems', 'description': \"This repository provides state of the art (SoTA) results for all machine learning problems. We do our best to keep this repository up to date.  If you do find a problem's SoTA result is out of date or missing, please raise this as an issue or submit Google form (with this information: research paper name, dataset, metric, source code and year). We will fix it immediately.\"}\n",
            "{'stars': 8957, 'url': 'https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT-RAG', 'description': 'Data-Centric FinGPT.  Open-source for open finance!  Revolutionize 🔥    We release the trained model on HuggingFace.'}\n",
            "{'stars': 8604, 'url': 'https://github.com/thudm/chatglm3', 'description': 'ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型'}\n",
            "{'stars': 8604, 'url': 'https://github.com/thudm/chatglm', 'description': 'ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型'}\n",
            "{'stars': 8603, 'url': 'https://github.com/weaviate/weaviate', 'description': 'Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance and scalability of a cloud-native database, all accessible through GraphQL, REST, and various language clients.'}\n",
            "{'stars': 8498, 'url': 'https://github.com/NVIDIA/TensorRT', 'description': 'NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT.'}\n",
            "{'stars': 8331, 'url': 'https://github.com/nebuly-ai/nebuly', 'description': 'The user analytics platform for LLMs'}\n",
            "{'stars': 8225, 'url': 'https://github.com/qwenlm/qwen', 'description': 'The official repo of Qwen (通义千问) chat & pretrained large language model proposed by Alibaba Cloud.'}\n",
            "{'stars': 7930, 'url': 'https://github.com/flagalpha/llama2-chinese', 'description': 'Llama中文社区，最好的中文Llama大模型，完全开源可商用'}\n",
            "{'stars': 7758, 'url': 'https://github.com/voicepaw/so-vits-svc-fork', 'description': 'so-vits-svc fork with realtime support, improved interface and more features.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratio_index = []\n",
        "\n",
        "laplation_factor = 1000\n",
        "\n",
        "for i,data in enumerate(datas):\n",
        "    if \"increased_stars\" in data:\n",
        "        ratio_value = (data[\"increased_stars\"])/(laplation_factor+data[\"stars\"])\n",
        "        # 计算ratio值\n",
        "        ratio_index.append((ratio_value, i))\n",
        "\n",
        "# 根据ratio排序\n",
        "ratio_index.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# 取前15个\n",
        "top15 = ratio_index[:20]\n",
        "\n",
        "# 打印data\n",
        "for ratio, index in top15:\n",
        "    print(round(ratio*1000)/10,datas[index])\n",
        "\n"
      ],
      "metadata": {
        "id": "VqKKo0-dtcuF",
        "outputId": "fa6c6088-f379-45f8-8f27-94e4acef4c08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.1 {'stars': 9198, 'url': 'https://github.com/graphdeco-inria/gaussian-splatting', 'description': 'Original reference implementation of \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"', 'increased_stars': 6537}\n",
            "58.0 {'stars': 7402, 'url': 'https://github.com/facebookresearch/nougat', 'description': 'Implementation of Nougat Neural Optical Understanding for Academic Documents', 'increased_stars': 4872}\n",
            "57.8 {'stars': 9808, 'url': 'https://github.com/hiyouga/llama-efficient-tuning', 'description': 'Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM2)', 'increased_stars': 6248}\n",
            "57.6 {'stars': 4581, 'url': 'https://github.com/vinthony/video-retalking', 'description': '[SIGGRAPH Asia 2022] VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild', 'increased_stars': 3214}\n",
            "56.2 {'stars': 12430, 'url': 'https://github.com/haotian-liu/LLaVA', 'description': 'Visual Instruction Tuning: Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities.', 'increased_stars': 7542}\n",
            "53.1 {'stars': 2627, 'url': 'https://github.com/flagopen/flagembedding', 'description': 'Open-source Embeddings', 'increased_stars': 1926}\n",
            "52.0 {'stars': 4407, 'url': 'https://github.com/imoneoi/openchat', 'description': 'OpenChat: Advancing Open-source Language Models with Imperfect Data', 'increased_stars': 2813}\n",
            "50.6 {'stars': 13291, 'url': 'https://github.com/vllm-project/vllm', 'description': 'A high-throughput and memory-efficient inference and serving engine for LLMs', 'increased_stars': 7229}\n",
            "50.5 {'stars': 1675, 'url': 'https://github.com/intel/intel-extension-for-transformers', 'description': '⚡Extending Hugging Face transformers APIs for Transformer-based models and improve the productivity of inference deployment. With extremely compressed models, the toolkit can greatly improve the inference efficiency on Intel platforms. ⚡', 'increased_stars': 1351}\n",
            "49.8 {'stars': 7059, 'url': 'https://github.com/guoyww/animatediff', 'description': 'Official implementation of AnimateDiff.', 'increased_stars': 4012}\n",
            "49.6 {'stars': 18680, 'url': 'https://github.com/stability-ai/generative-models', 'description': 'Generative Models by Stability AI', 'increased_stars': 9768}\n",
            "48.8 {'stars': 8225, 'url': 'https://github.com/QwenLM/Qwen-7B', 'description': 'The official repo of Qwen-7B (通义千问-7B) chat & pretrained large language model proposed by Alibaba Cloud.', 'increased_stars': 4501}\n",
            "46.2 {'stars': 9568, 'url': 'https://github.com/lukas-blecher/LaTeX-OCR', 'description': 'pix2tex: Using a ViT to convert images of equations into LaTeX code.', 'increased_stars': 4887}\n",
            "45.9 {'stars': 22367, 'url': 'https://github.com/comfyanonymous/comfyui', 'description': 'A powerful and modular stable diffusion GUI with a graph/nodes interface.', 'increased_stars': 10726}\n",
            "45.0 {'stars': 4741, 'url': 'https://github.com/stanfordnlp/dsp', 'description': 'DSPy: The framework for programming with foundation models', 'increased_stars': 2586}\n",
            "44.3 {'stars': 9380, 'url': 'https://github.com/facebookresearch/seamless_communication', 'description': 'Foundational Models for State-of-the-Art Speech and Text Translation', 'increased_stars': 4597}\n",
            "42.7 {'stars': 6115, 'url': 'https://github.com/thudm/codegeex2', 'description': 'CodeGeeX2: A More Powerful Multilingual Code Generation Model', 'increased_stars': 3040}\n",
            "42.2 {'stars': 1805, 'url': 'https://github.com/paitesanshi/llm-agent-survey', 'description': '', 'increased_stars': 1184}\n",
            "40.1 {'stars': 2291, 'url': 'https://github.com/qwenlm/qwen-vl', 'description': 'The official repo of Qwen-VL (通义千问-VL) chat & pretrained large vision language model proposed by Alibaba Cloud.', 'increased_stars': 1320}\n",
            "38.8 {'stars': 5685, 'url': 'https://github.com/ymcui/chinese-llama-alpaca-2', 'description': '中文LLaMA-2 & Alpaca-2大模型二期项目 + 16K超长上下文模型 (Chinese LLaMA-2 & Alpaca-2 LLMs, including 16K long context models)', 'increased_stars': 2591}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "# 假设 ratio 是你已经填充好的列表\n",
        "# 获取最大的15个元素\n",
        "top_15_elements = heapq.nlargest(15, ratio)\n"
      ],
      "metadata": {
        "id": "ARujZwjdt5xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}