{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN4T8YoSfqNP6+iYU4Fvp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/awesome-colab-project/blob/main/NewPaperWithCodeSpider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ–°ç‰ˆçš„paper with codeçˆ¬è™«ï¼Œå¢žåŠ GPTè‡ªåŠ¨æ€»ç»“çš„åŠŸèƒ½\n",
        "\n",
        "implemented by æŽé²é²"
      ],
      "metadata": {
        "id": "3yCj2XI0GyOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q paperswithcode-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftqKbG45Hwx1",
        "outputId": "1ab5228a-e7aa-4db7-86fc-46687d7cef57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m503.5/503.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m195.6/195.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m470.9/470.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m674.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "sqlalchemy 2.0.24 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "confection 0.1.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "flax 0.7.5 requires rich>=11.1, but you have rich 9.11.1 which is incompatible.\n",
            "flax 0.7.5 requires typing-extensions>=4.2, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "ibis-framework 7.1.0 requires pytz>=2022.7, but you have pytz 2021.3 which is incompatible.\n",
            "ibis-framework 7.1.0 requires rich<14,>=12.4.4, but you have rich 9.11.1 which is incompatible.\n",
            "ibis-framework 7.1.0 requires typing-extensions<5,>=4.3.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.6.2 which is incompatible.\n",
            "librosa 0.10.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\n",
            "polars 0.17.3 requires typing_extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "python-utils 3.8.1 requires typing-extensions>3.10.0.2, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "spacy 3.6.1 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
            "thinc 8.1.12 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
            "yfinance 0.2.33 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_name = '/content/drive/MyDrive/paper_with_code.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTmHAYdmKpuv",
        "outputId": "3e935e35-566a-4bf3-9664-983ef5f32833"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "previous_datas = []\n",
        "\n",
        "with open(save_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        previous_datas.append(json.loads(line))\n",
        "        tmp = json.loads(line)\n",
        "        visited_urls.add(tmp['url'])\n",
        "\n",
        "minimal_stars = previous_datas[-1]['stars']\n"
      ],
      "metadata": {
        "id": "M1nJ9FYXSe5p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(previous_datas[-1].keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L5lIGzPSumy",
        "outputId": "2a26160c-31be-472b-a5da-e7622349c858"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['stars', 'url', 'description'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/paper_with_code"
      ],
      "metadata": {
        "id": "AKt15WpZUbJp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url2id = {}\n",
        "\n",
        "for i, data in enumerate(previous_datas):\n",
        "    url2id[data['url']] = i"
      ],
      "metadata": {
        "id": "_0oTS1WuUqJ8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from paperswithcode import PapersWithCodeClient\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "client = PapersWithCodeClient()\n",
        "\n",
        "save_name = '/content/drive/MyDrive/paper_with_code/0106.txt'\n",
        "# save_name = '/content/output.txt'\n",
        "\n",
        "\n",
        "written_in_this_time_scan = set()\n",
        "for page_id in range(1,300):\n",
        "    if page_id == 1:\n",
        "        open_format = 'w'\n",
        "    else:\n",
        "        open_format = 'a'\n",
        "    with open(save_name, open_format, encoding='utf-8') as f:\n",
        "\n",
        "        papers = client.repository_list(stars=50, items_per_page=500, page=page_id)\n",
        "\n",
        "        print('dealing with page ', page_id)\n",
        "\n",
        "\n",
        "        for data in tqdm(papers.results):\n",
        "            my_dict = {}\n",
        "            my_dict['stars'] = data.stars\n",
        "\n",
        "            my_dict['url'] = data.url\n",
        "\n",
        "            if data.url in written_in_this_time_scan:\n",
        "                continue\n",
        "\n",
        "            if data.url in url2id:\n",
        "                new_stars = data.stars\n",
        "                index = url2id[data.url]\n",
        "                old_stars = previous_datas[index]['stars']\n",
        "                if new_stars > old_stars:\n",
        "                    previous_datas[index]['stars'] = new_stars\n",
        "                    previous_datas[index]['increased_stars'] = new_stars - old_stars\n",
        "                json_str = json.dumps(previous_datas[index], ensure_ascii=False)\n",
        "                f.write(json_str+'\\n')\n",
        "                written_in_this_time_scan.add(data.url)\n",
        "                continue\n",
        "\n",
        "\n",
        "            written_in_this_time_scan.add(data.url)\n",
        "\n",
        "            page = requests.get(data.url)\n",
        "\n",
        "            if 'colab' not in page.text and 'Colab' not in page.text and 'HuggingFace' not in page.text and 'huggingface' not in page.text:\n",
        "                continue\n",
        "\n",
        "            my_dict['description'] = data.description\n",
        "            json_str = json.dumps(my_dict, ensure_ascii=False)\n",
        "            f.write(json_str+'\\n')"
      ],
      "metadata": {
        "id": "Ri6PQrMFNTva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9V0hp6TTVoj",
        "outputId": "ea3c4a65-73af-4b68-fc12-ffde39b67004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/kornia/kornia/blob/master/kornia/feature/siftdesc.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(papers.results[1])\n",
        "print(papers.results[1].url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPz8Mz35Ncch",
        "outputId": "7e654566-1d2e-40cb-b01c-57603674445f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url='https://github.com/sindresorhus/awesome' owner='sindresorhus' name='awesome' description='ðŸ˜Ž Awesome lists about all kinds of interesting topics' stars=268030 framework='tf' is_official=None\n",
            "https://github.com/sindresorhus/awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from paperswithcode import PapersWithCodeClient\n",
        "import json\n",
        "\n",
        "client = PapersWithCodeClient()\n",
        "\n",
        "# save_name = '/content/output.txt'\n",
        "\n",
        "with open(save_name, 'w', encoding='utf-8') as f:\n",
        "\n",
        "  for page_id in range(1,200):\n",
        "\n",
        "    papers = client.repository_list(stars=100, items_per_page=500, page=page_id)\n",
        "\n",
        "    print('dealing page ', page_id)\n",
        "\n",
        "    for i in range(len(papers.results)):\n",
        "\n",
        "      res = papers.results[i]\n",
        "\n",
        "      try_count = 0\n",
        "      paper_list = []\n",
        "\n",
        "      while try_count < 3:\n",
        "\n",
        "        try:\n",
        "          paper_list = client.repository_paper_list(owner=res.owner, name=res.name)\n",
        "          time.sleep(0.1)\n",
        "          try_count += 10\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        if paper_list != []:\n",
        "          break\n",
        "\n",
        "      if not paper_list:\n",
        "        continue\n",
        "\n",
        "      my_url = getattr(papers.results[i],'url','')\n",
        "      page = requests.get(my_url)\n",
        "\n",
        "      if 'colab' not in page.text and 'Colab' not in page.text:\n",
        "        continue\n",
        "\n",
        "      my_dict = {}\n",
        "      my_dict['stars'] = getattr(papers.results[i],'stars','')\n",
        "      my_dict['name'] = getattr(papers.results[i],'name','')\n",
        "      my_dict['url'] = getattr(papers.results[i],'url','')\n",
        "      my_dict['description'] = getattr(papers.results[i],'description','')\n",
        "      json_str = json.dumps(my_dict)\n",
        "      f.write(json_str+'\\n')\n",
        "      print(json_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJqzUmX4I3Y6",
        "outputId": "a50daf02-e1b3-40b7-911f-0af0b877c67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dealing page  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G961dVE8KEns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç»™å‡ºtop15çš„åˆ†æž"
      ],
      "metadata": {
        "id": "nzK9sjdptNjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "datas = []\n",
        "\n",
        "with open(save_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        datas.append(json.loads(line))\n"
      ],
      "metadata": {
        "id": "N5vM7p7etO22"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(datas))"
      ],
      "metadata": {
        "id": "GYbhZKKCtbp0",
        "outputId": "340b3dae-f4ed-4b6f-a29a-16420b6701d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9D2L5IWDuwpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_projs = []\n",
        "\n",
        "laplation_factor = 1000\n",
        "\n",
        "for i,data in enumerate(datas):\n",
        "    if \"increased_stars\" not in data:\n",
        "        # è®¡ç®—ratioå€¼\n",
        "        new_projs.append((data[\"stars\"], i))\n",
        "\n",
        "# æ ¹æ®ratioæŽ’åº\n",
        "new_projs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# å–å‰15ä¸ª\n",
        "top15 = new_projs[:45]\n",
        "\n",
        "# æ‰“å°data\n",
        "for ratio, index in top15:\n",
        "    print(datas[index])\n",
        "\n"
      ],
      "metadata": {
        "id": "mb-UsKCHu87a",
        "outputId": "2da4519d-79b7-4397-a04b-494514b6890b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'stars': 114851, 'url': 'https://github.com/avelino/awesome-go', 'description': 'A curated list of awesome Go frameworks, libraries and software'}\n",
            "{'stars': 62273, 'url': 'https://github.com/josephmisiti/awesome-machine-learning', 'description': 'A curated list of awesome Machine Learning frameworks, libraries and software.'}\n",
            "{'stars': 60180, 'url': 'https://github.com/keras-team/keras', 'description': 'Deep Learning for humans'}\n",
            "{'stars': 60180, 'url': 'https://github.com/fchollet/keras', 'description': 'Deep Learning for humans'}\n",
            "{'stars': 44938, 'url': 'https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md', 'description': 'ðŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 44936, 'url': 'https://github.com/huggingface/transformers/tree/master/examples/question-answering', 'description': 'ðŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 37934, 'url': 'https://github.com/huggingface/transformers/tree/master/examples/movement-pruning', 'description': 'ðŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 37859, 'url': 'https://github.com/killianlucas/open-interpreter', 'description': 'A natural language interface for computers'}\n",
            "{'stars': 36726, 'url': 'https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_squeezebert.py', 'description': 'ðŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.'}\n",
            "{'stars': 33342, 'url': 'https://github.com/geekan/metagpt', 'description': 'ðŸŒŸ The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo'}\n",
            "{'stars': 31911, 'url': 'https://github.com/google-research/google-research/tree/master/blur', 'description': 'Google Research'}\n",
            "{'stars': 31906, 'url': 'https://github.com/google-research/google-research/tree/master/muller', 'description': 'Google Research'}\n",
            "{'stars': 31906, 'url': 'https://github.com/google-research/google-research/tree/master/memory_efficient_attention', 'description': 'Google Research'}\n",
            "{'stars': 28741, 'url': 'https://github.com/lllyasviel/fooocus', 'description': 'Focus on prompting and generating'}\n",
            "{'stars': 26751, 'url': 'https://github.com/jerryjliu/gpt_index', 'description': 'LlamaIndex (formerly GPT Index) is a data framework for your LLM applications'}\n",
            "{'stars': 26751, 'url': 'https://github.com/jerryjliu/llama_index', 'description': 'LlamaIndex (formerly GPT Index) is a data framework for your LLM applications'}\n",
            "{'stars': 26744, 'url': 'https://github.com/run-llama/llama_index', 'description': 'LlamaIndex (formerly GPT Index) is a data framework for your LLM applications'}\n",
            "{'stars': 25751, 'url': 'https://github.com/coqui-ai/TTS', 'description': 'ðŸ¸ðŸ’¬ - a deep learning toolkit for Text-to-Speech, battle-tested in research and production'}\n",
            "{'stars': 25343, 'url': 'https://github.com/openbb-finance/openbbterminal', 'description': 'Investment Research for Everyone, Everywhere.'}\n",
            "{'stars': 21113, 'url': 'https://github.com/chatchat-space/langchain-chatchat', 'description': 'Langchain-Chatchatï¼ˆåŽŸLangchain-ChatGLMï¼‰åŸºäºŽ Langchain ä¸Ž ChatGLM ç­‰è¯­è¨€æ¨¡åž‹çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­” | Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain '}\n",
            "{'stars': 19605, 'url': 'https://github.com/mindsdb/mindsdb', 'description': 'Build AI ðŸ¤– using SQL'}\n",
            "{'stars': 15578, 'url': 'https://github.com/apple/ml-stable-diffusion', 'description': 'Stable Diffusion with Core ML on Apple Silicon'}\n",
            "{'stars': 14182, 'url': 'https://github.com/huggingface/datasets/tree/master/datasets/cppe-5', 'description': 'ðŸ¤— The largest hub of ready-to-use datasets for ML models with fast, easy-to-use and efficient data manipulation tools'}\n",
            "{'stars': 14162, 'url': 'https://github.com/logspace-ai/langflow', 'description': 'â›“ï¸ Langflow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.'}\n",
            "{'stars': 13885, 'url': 'https://github.com/karpathy/llama2.c', 'description': 'Inference Llama 2 in one file of pure C'}\n",
            "{'stars': 13532, 'url': 'https://github.com/w-okada/voice-changer', 'description': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒœã‚¤ã‚¹ãƒã‚§ãƒ³ã‚¸ãƒ£ãƒ¼ Realtime Voice Changer'}\n",
            "{'stars': 13001, 'url': 'https://github.com/openai/evals', 'description': 'Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.'}\n",
            "{'stars': 12595, 'url': 'https://github.com/sunner/chatall', 'description': ' Concurrently chat with ChatGPT, Bing Chat, Bard, Alpaca, Vicuna, Claude, ChatGLM, MOSS, è®¯é£žæ˜Ÿç«, æ–‡å¿ƒä¸€è¨€ and more, discover the best answers'}\n",
            "{'stars': 12390, 'url': 'https://github.com/myshell-ai/openvoice', 'description': 'Instant voice cloning by MyShell.'}\n",
            "{'stars': 11677, 'url': 'https://github.com/ydataai/ydata-profiling', 'description': '1 Line of code data quality profiling & exploratory data analysis for Pandas and Spark DataFrames. '}\n",
            "{'stars': 11293, 'url': 'https://github.com/hannibal046/awesome-llm', 'description': 'Awesome-LLM: a curated list of Large Language Model'}\n",
            "{'stars': 11236, 'url': 'https://github.com/google-deepmind/alphafold', 'description': 'Open source code for AlphaFold.'}\n",
            "{'stars': 9808, 'url': 'https://github.com/hiyouga/llama-factory', 'description': 'Easy-to-use LLM fine-tuning framework (LLaMA, BLOOM, Mistral, Baichuan, Qwen, ChatGLM)'}\n",
            "{'stars': 9469, 'url': 'https://github.com/gventuri/pandas-ai', 'description': 'Chat with your data (CSV, pandas, polars, etc). PandasAI makes data analysis conversational'}\n",
            "{'stars': 9371, 'url': 'https://github.com/eosphoros-ai/db-gpt', 'description': 'Revolutionizing Database Interactions with Private LLM Technology'}\n",
            "{'stars': 8961, 'url': 'https://github.com/redditsota/state-of-the-art-result-for-machine-learning-problems', 'description': \"This repository provides state of the art (SoTA) results for all machine learning problems. We do our best to keep this repository up to date.  If you do find a problem's SoTA result is out of date or missing, please raise this as an issue or submit Google form (with this information: research paper name, dataset, metric, source code and year). We will fix it immediately.\"}\n",
            "{'stars': 8957, 'url': 'https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT-RAG', 'description': 'Data-Centric FinGPT.  Open-source for open finance!  Revolutionize ðŸ”¥    We release the trained model on HuggingFace.'}\n",
            "{'stars': 8604, 'url': 'https://github.com/thudm/chatglm3', 'description': 'ChatGLM3 series: Open Bilingual Chat LLMs | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡åž‹'}\n",
            "{'stars': 8604, 'url': 'https://github.com/thudm/chatglm', 'description': 'ChatGLM3 series: Open Bilingual Chat LLMs | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡åž‹'}\n",
            "{'stars': 8603, 'url': 'https://github.com/weaviate/weaviate', 'description': 'Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance and scalability of a cloud-native database, all accessible through GraphQL, REST, and various language clients.'}\n",
            "{'stars': 8498, 'url': 'https://github.com/NVIDIA/TensorRT', 'description': 'NVIDIAÂ® TensorRTâ„¢ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT.'}\n",
            "{'stars': 8331, 'url': 'https://github.com/nebuly-ai/nebuly', 'description': 'The user analytics platform for LLMs'}\n",
            "{'stars': 8225, 'url': 'https://github.com/qwenlm/qwen', 'description': 'The official repo of Qwen (é€šä¹‰åƒé—®) chat & pretrained large language model proposed by Alibaba Cloud.'}\n",
            "{'stars': 7930, 'url': 'https://github.com/flagalpha/llama2-chinese', 'description': 'Llamaä¸­æ–‡ç¤¾åŒºï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡åž‹ï¼Œå®Œå…¨å¼€æºå¯å•†ç”¨'}\n",
            "{'stars': 7758, 'url': 'https://github.com/voicepaw/so-vits-svc-fork', 'description': 'so-vits-svc fork with realtime support, improved interface and more features.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratio_index = []\n",
        "\n",
        "laplation_factor = 1000\n",
        "\n",
        "for i,data in enumerate(datas):\n",
        "    if \"increased_stars\" in data:\n",
        "        ratio_value = (data[\"increased_stars\"])/(laplation_factor+data[\"stars\"])\n",
        "        # è®¡ç®—ratioå€¼\n",
        "        ratio_index.append((ratio_value, i))\n",
        "\n",
        "# æ ¹æ®ratioæŽ’åº\n",
        "ratio_index.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# å–å‰15ä¸ª\n",
        "top15 = ratio_index[:20]\n",
        "\n",
        "# æ‰“å°data\n",
        "for ratio, index in top15:\n",
        "    print(round(ratio*1000)/10,datas[index])\n",
        "\n"
      ],
      "metadata": {
        "id": "VqKKo0-dtcuF",
        "outputId": "fa6c6088-f379-45f8-8f27-94e4acef4c08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.1 {'stars': 9198, 'url': 'https://github.com/graphdeco-inria/gaussian-splatting', 'description': 'Original reference implementation of \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"', 'increased_stars': 6537}\n",
            "58.0 {'stars': 7402, 'url': 'https://github.com/facebookresearch/nougat', 'description': 'Implementation of Nougat Neural Optical Understanding for Academic Documents', 'increased_stars': 4872}\n",
            "57.8 {'stars': 9808, 'url': 'https://github.com/hiyouga/llama-efficient-tuning', 'description': 'Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM2)', 'increased_stars': 6248}\n",
            "57.6 {'stars': 4581, 'url': 'https://github.com/vinthony/video-retalking', 'description': '[SIGGRAPH Asia 2022] VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild', 'increased_stars': 3214}\n",
            "56.2 {'stars': 12430, 'url': 'https://github.com/haotian-liu/LLaVA', 'description': 'Visual Instruction Tuning: Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities.', 'increased_stars': 7542}\n",
            "53.1 {'stars': 2627, 'url': 'https://github.com/flagopen/flagembedding', 'description': 'Open-source Embeddings', 'increased_stars': 1926}\n",
            "52.0 {'stars': 4407, 'url': 'https://github.com/imoneoi/openchat', 'description': 'OpenChat: Advancing Open-source Language Models with Imperfect Data', 'increased_stars': 2813}\n",
            "50.6 {'stars': 13291, 'url': 'https://github.com/vllm-project/vllm', 'description': 'A high-throughput and memory-efficient inference and serving engine for LLMs', 'increased_stars': 7229}\n",
            "50.5 {'stars': 1675, 'url': 'https://github.com/intel/intel-extension-for-transformers', 'description': 'âš¡Extending Hugging Face transformers APIs for Transformer-based models and improve the productivity of inference deployment. With extremely compressed models, the toolkit can greatly improve the inference efficiency on Intel platforms. âš¡', 'increased_stars': 1351}\n",
            "49.8 {'stars': 7059, 'url': 'https://github.com/guoyww/animatediff', 'description': 'Official implementation of AnimateDiff.', 'increased_stars': 4012}\n",
            "49.6 {'stars': 18680, 'url': 'https://github.com/stability-ai/generative-models', 'description': 'Generative Models by Stability AI', 'increased_stars': 9768}\n",
            "48.8 {'stars': 8225, 'url': 'https://github.com/QwenLM/Qwen-7B', 'description': 'The official repo of Qwen-7B (é€šä¹‰åƒé—®-7B) chat & pretrained large language model proposed by Alibaba Cloud.', 'increased_stars': 4501}\n",
            "46.2 {'stars': 9568, 'url': 'https://github.com/lukas-blecher/LaTeX-OCR', 'description': 'pix2tex: Using a ViT to convert images of equations into LaTeX code.', 'increased_stars': 4887}\n",
            "45.9 {'stars': 22367, 'url': 'https://github.com/comfyanonymous/comfyui', 'description': 'A powerful and modular stable diffusion GUI with a graph/nodes interface.', 'increased_stars': 10726}\n",
            "45.0 {'stars': 4741, 'url': 'https://github.com/stanfordnlp/dsp', 'description': 'DSPy: The framework for programming with foundation models', 'increased_stars': 2586}\n",
            "44.3 {'stars': 9380, 'url': 'https://github.com/facebookresearch/seamless_communication', 'description': 'Foundational Models for State-of-the-Art Speech and Text Translation', 'increased_stars': 4597}\n",
            "42.7 {'stars': 6115, 'url': 'https://github.com/thudm/codegeex2', 'description': 'CodeGeeX2: A More Powerful Multilingual Code Generation Model', 'increased_stars': 3040}\n",
            "42.2 {'stars': 1805, 'url': 'https://github.com/paitesanshi/llm-agent-survey', 'description': '', 'increased_stars': 1184}\n",
            "40.1 {'stars': 2291, 'url': 'https://github.com/qwenlm/qwen-vl', 'description': 'The official repo of Qwen-VL (é€šä¹‰åƒé—®-VL) chat & pretrained large vision language model proposed by Alibaba Cloud.', 'increased_stars': 1320}\n",
            "38.8 {'stars': 5685, 'url': 'https://github.com/ymcui/chinese-llama-alpaca-2', 'description': 'ä¸­æ–‡LLaMA-2 & Alpaca-2å¤§æ¨¡åž‹äºŒæœŸé¡¹ç›® + 16Kè¶…é•¿ä¸Šä¸‹æ–‡æ¨¡åž‹ (Chinese LLaMA-2 & Alpaca-2 LLMs, including 16K long context models)', 'increased_stars': 2591}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "# å‡è®¾ ratio æ˜¯ä½ å·²ç»å¡«å……å¥½çš„åˆ—è¡¨\n",
        "# èŽ·å–æœ€å¤§çš„15ä¸ªå…ƒç´ \n",
        "top_15_elements = heapq.nlargest(15, ratio)\n"
      ],
      "metadata": {
        "id": "ARujZwjdt5xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}